"""Sistema RAG Multi-Agente con B√∫squeda H√≠brida (Sem√°ntica y L√©xica)

Este m√≥dulo implementa un sistema de Retrieval-Augmented Generation (RAG) 
especializado para la aplicaci√≥n ScanGasto. El sistema utiliza una arquitectura
multi-agente con capacidades de b√∫squeda h√≠brida:

- B√∫squeda Sem√°ntica: Utiliza embeddings y ChromaDB para b√∫squedas conceptuales
- B√∫squeda L√©xica: Realiza b√∫squedas literales en archivos markdown

Componentes principales:
    - Agente Orquestador: Clasifica preguntas por categor√≠a y tipo de b√∫squeda
    - Agentes Especializados: Funcional, T√©cnico y Gesti√≥n
    - Agente Sintetizador: Fusiona respuestas de m√∫ltiples agentes
    - Interfaz Gradio: UI web interactiva con opciones configurables

Autor: Equipo Capstone IIA
Versi√≥n: 2.0
Fecha: Diciembre 2025
"""

import os
import re
import glob
import gradio as gr
from dotenv import load_dotenv
import chromadb
from chromadb.utils import embedding_functions
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

# Cargar variables de entorno
load_dotenv()

# Verificar API KEY
API_KEY = os.getenv("OPENAI_API_KEY")
if not API_KEY:
    raise ValueError("‚ùå No se encontr√≥ la variable OPENAI_API_KEY. Configura tu archivo .env")

# Configuraci√≥n de ChromaDB
DB_PATH = './bbdd'
COLLECTION_NAME = "documentacion_openai"
MODEL_NAME = "text-embedding-3-small"

# Regex compilados para extraer informaci√≥n de clasificaci√≥n
CATEGORIA_PATTERN = re.compile(r'Categor√≠a:\s*(FUNCIONAL|TECNICA|GESTION)', re.IGNORECASE)
TIPO_BUSQUEDA_PATTERN = re.compile(r'Tipo de b√∫squeda:\s*(SEMANTICA|LEXICA)', re.IGNORECASE)

# Cliente LangChain global (reutilizable)
llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.3,
    api_key=API_KEY
)

# Cache de la colecci√≥n ChromaDB
_collection_cache = None

def get_chroma_collection():
    """Obtiene la colecci√≥n de ChromaDB con patr√≥n Singleton.
    
    Implementa un patr√≥n de cach√© para evitar reconexiones innecesarias
    a la base de datos vectorial ChromaDB. La colecci√≥n se inicializa
    una sola vez y se reutiliza en llamadas posteriores.
    
    Returns:
        chromadb.Collection: Colecci√≥n de ChromaDB configurada con
            funci√≥n de embeddings de OpenAI (text-embedding-3-small)
    
    Note:
        Utiliza la variable global _collection_cache para persistencia
    """
    global _collection_cache
    
    if _collection_cache is None:
        chroma_client = chromadb.PersistentClient(path=DB_PATH)
        openai_ef = embedding_functions.OpenAIEmbeddingFunction(
            api_key=API_KEY,
            model_name=MODEL_NAME
        )
        _collection_cache = chroma_client.get_or_create_collection(
            name=COLLECTION_NAME,
            embedding_function=openai_ef
        )
    
    return _collection_cache

def agente_orquestador(pregunta):
    """Agente Orquestador: Clasifica preguntas en dos dimensiones.
    
    Este agente es el punto de entrada del sistema multi-agente. Utiliza
    GPT-4o-mini para realizar una doble clasificaci√≥n de la pregunta del usuario:
    
    1. Categor√≠a de dominio:
       - FUNCIONAL: Funcionalidades, caracter√≠sticas, comportamiento de usuario
       - TECNICA: Implementaci√≥n, c√≥digo, arquitectura, tecnolog√≠as
       - GESTION: Procesos, organizaci√≥n, documentaci√≥n, planificaci√≥n
    
    2. Tipo de b√∫squeda:
       - SEMANTICA: B√∫squedas conceptuales que requieren comprensi√≥n contextual
       - LEXICA: B√∫squedas literales de t√©rminos, campos o variables espec√≠ficas
    
    Args:
        pregunta (str): Pregunta del usuario a clasificar
    
    Returns:
        str: Texto con la clasificaci√≥n en formato estructurado:
             "Categor√≠a: [CATEGORIA]\nTipo de b√∫squeda: [TIPO]\nJustificaci√≥n: [TEXTO]"
             En caso de error, retorna mensaje de error con el detalle
    
    Example:
        >>> agente_orquestador("¬øC√≥mo funciona el sistema de QR?")
        "Categor√≠a: FUNCIONAL\nTipo de b√∫squeda: SEMANTICA\n..."
    """
    template = """Eres un agente clasificador experto. Tu tarea es analizar preguntas y hacer dos clasificaciones:

**CATEGOR√çA (elige una):**
1. FUNCIONAL: Preguntas sobre c√≥mo funciona algo, caracter√≠sticas, comportamiento de usuario, casos de uso, flujos de trabajo.
2. TECNICA: Preguntas sobre implementaci√≥n, c√≥digo, arquitectura, tecnolog√≠as, APIs, bases de datos, desarrollo.
3. GESTION: Preguntas sobre procesos, organizaci√≥n, documentaci√≥n, planificaci√≥n, administraci√≥n, procedimientos.

**TIPO DE B√öSQUEDA (elige uno):**
- SEMANTICA: Preguntas conceptuales, de comprensi√≥n, que requieren entender el significado y contexto. Ejemplos: "¬øc√≥mo funciona X?", "¬øqu√© hace Y?", "¬øpara qu√© sirve Z?"
- LEXICA: B√∫squedas de t√©rminos espec√≠ficos, nombres exactos de campos, variables, strings, o ubicaciones de c√≥digo. Ejemplos: "¬ød√≥nde aparece el campo X?", "¬øen qu√© archivo est√° la variable Y?", "busca el string Z"

Responde √öNICAMENTE con el formato:
Categor√≠a: [FUNCIONAL/TECNICA/GESTION]
Tipo de b√∫squeda: [SEMANTICA/LEXICA]
Justificaci√≥n: [Breve explicaci√≥n]

Pregunta: {pregunta}"""

    try:
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | llm
        response = chain.invoke({"pregunta": pregunta})
        return response.content
    except Exception as e:
        return f"‚ùå Error al clasificar la pregunta: {str(e)}"

def extraer_categoria(clasificacion_texto):
    """Extrae la categor√≠a del texto de clasificaci√≥n usando regex.
    
    Args:
        clasificacion_texto (str): Texto con formato "Categor√≠a: [CATEGORIA]"
    
    Returns:
        str: Categor√≠a extra√≠da en may√∫sculas (FUNCIONAL/TECNICA/GESTION)
             o "DESCONOCIDA" si no se encuentra patr√≥n v√°lido
    """
    match = CATEGORIA_PATTERN.search(clasificacion_texto)
    if match:
        return match.group(1).upper()
    return "DESCONOCIDA"

def extraer_tipo_busqueda(clasificacion_texto):
    """Extrae el tipo de b√∫squeda del texto de clasificaci√≥n usando regex.
    
    Args:
        clasificacion_texto (str): Texto con formato "Tipo de b√∫squeda: [TIPO]"
    
    Returns:
        str: Tipo de b√∫squeda extra√≠do (SEMANTICA/LEXICA)
             Por defecto retorna "SEMANTICA" si no se encuentra patr√≥n
    """
    match = TIPO_BUSQUEDA_PATTERN.search(clasificacion_texto)
    if match:
        return match.group(1).upper()
    return "SEMANTICA"  # Por defecto, asumimos b√∫squeda sem√°ntica

def buscar_documentos_relevantes(pregunta, categoria, n_results=3):
    """Busca documentos relevantes en ChromaDB seg√∫n la pregunta y categor√≠a."""
    collection = get_chroma_collection()
    results = collection.query(
        query_texts=[pregunta],
        n_results=n_results,
        where={"category": categoria} if categoria != "DESCONOCIDA" else None
    )
    return results

def construir_contexto(documentos, metadatas):
    """Construye el contexto a partir de documentos y metadatos."""
    contexto_partes = [
        f"[Documento {i} - {meta.get('source_file', 'Desconocido')}]\n{doc}"
        for i, (doc, meta) in enumerate(zip(documentos, metadatas), 1)
    ]
    return "\n\n---\n\n".join(contexto_partes)

def formatear_respuesta_con_fuentes(contenido, metadatas, mostrar_fuentes=True):
    """Formatea la respuesta incluyendo opcionalmente las fuentes consultadas."""
    if not mostrar_fuentes:
        return contenido
    
    fuentes_unicas = list(dict.fromkeys(
        meta.get('source_file', 'Desconocido') for meta in metadatas
    ))
    fuentes_str = "\n".join(f"- {fuente}" for fuente in fuentes_unicas)
    return f"{contenido}\n\nüìö **Fuentes consultadas:**\n{fuentes_str}"

def busqueda_lexica_en_archivos(pregunta, carpeta_docs):
    """Realiza una b√∫squeda l√©xica (texto literal) en archivos markdown."""
    # Extraer posibles t√©rminos de b√∫squeda de la pregunta
    terminos = []
    
    # 1. Buscar texto entre comillas
    quoted = re.findall(r'["\'""]([^"\'\'"]+)["\'""]', pregunta)
    terminos.extend(quoted)
    
    # 2. Si no hay texto entre comillas, buscar palabras espec√≠ficas/t√©cnicas
    if not terminos:
        # Palabras comunes a excluir
        palabras_excluidas = {
            'el', 'la', 'los', 'las', 'un', 'una', 'de', 'en', 'y', 'o', 'que', 'se', 
            'donde', 'como', 'cual', 'este', 'esta', 'es', 'son', 'aparece', 
            'esta', 'hay', 'tiene', 'busca', 'encuentra', 'campo', 'variable',
            'string', 'funcion', 'metodo', 'archivo', 'documento', 'codigo', 
            'documentaci√≥n', 'documentacion'
        }
        
        palabras = re.findall(r'\b\w+\b', pregunta.lower())
        candidatos = [p for p in palabras if len(p) > 3 and p not in palabras_excluidas]
        
        # Priorizar t√©rminos t√©cnicos (con _, -, o camelCase)
        terminos_tecnicos = [c for c in candidatos if '_' in c or '-' in c]
        
        if terminos_tecnicos:
            # Si hay t√©rminos t√©cnicos, usar solo el m√°s largo (m√°s espec√≠fico)
            terminos = [max(terminos_tecnicos, key=len)]
        elif candidatos:
            # Si no hay t√©rminos t√©cnicos, usar el m√°s largo
            terminos = [max(candidatos, key=len)]
    
    if not terminos:
        return None, []
    
    # Buscar en archivos markdown
    patron_archivos = os.path.join(carpeta_docs, '*.md')
    archivos = glob.glob(patron_archivos)
    
    resultados = []
    for archivo in archivos:
        try:
            with open(archivo, 'r', encoding='utf-8') as f:
                contenido = f.read()
                lineas = contenido.split('\n')
                
                # Buscar coincidencias
                for i, linea in enumerate(lineas, 1):
                    for termino in terminos:
                        if termino.lower() in linea.lower():
                            # Contexto: l√≠nea anterior y siguiente
                            contexto_inicio = max(0, i - 2)
                            contexto_fin = min(len(lineas), i + 1)
                            contexto = '\n'.join(lineas[contexto_inicio:contexto_fin])
                            
                            resultados.append({
                                'archivo': os.path.basename(archivo),
                                'linea': i,
                                'termino': termino,
                                'contexto': contexto
                            })
                            break  # Solo una coincidencia por l√≠nea
        except Exception as e:
            continue
    
    return terminos, resultados

def formatear_resultados_lexicos(terminos, resultados, mostrar_fuentes):
    """Formatea los resultados de una b√∫squeda l√©xica."""
    if not resultados:
        return f"‚ö†Ô∏è No se encontraron coincidencias para los t√©rminos buscados: {', '.join(terminos)}"
    
    # Agrupar por archivo
    por_archivo = {}
    for r in resultados:
        archivo = r['archivo']
        if archivo not in por_archivo:
            por_archivo[archivo] = []
        por_archivo[archivo].append(r)
    
    # Formatear respuesta
    respuesta = f"üîç **B√∫squeda l√©xica de:** {', '.join(terminos)}\n\n"
    respuesta += f"Se encontraron **{len(resultados)} coincidencias** en **{len(por_archivo)} archivos**:\n\n"
    
    for archivo, coincidencias in por_archivo.items():
        respuesta += f"### üìÑ {archivo}\n"
        for c in coincidencias[:3]:  # Limitar a 3 coincidencias por archivo
            respuesta += f"\n**L√≠nea {c['linea']}:**\n```\n{c['contexto']}\n```\n"
        
        if len(coincidencias) > 3:
            respuesta += f"\n_... y {len(coincidencias) - 3} coincidencias m√°s en este archivo_\n"
        respuesta += "\n"
    
    if mostrar_fuentes:
        fuentes = list(por_archivo.keys())
        respuesta += f"\nüìö **Archivos consultados:**\n"
        respuesta += "\n".join(f"- {f}" for f in fuentes)
    
    return respuesta

def agente_funcional(pregunta, categoria, tipo_busqueda, mostrar_fuentes=True):
    """
    Agente funcional que busca documentos relevantes en la BBDD vectorial (sem√°ntica)
    o realiza b√∫squeda l√©xica en archivos markdown seg√∫n el tipo de b√∫squeda.
    
    Args:
        pregunta: La pregunta del usuario
        categoria: Categor√≠a de la pregunta (FUNCIONAL/TECNICA/GESTION)
        tipo_busqueda: Tipo de b√∫squeda (SEMANTICA/LEXICA)
        mostrar_fuentes: Si se deben mostrar las fuentes consultadas
    """
    try:
        # Manejo de b√∫squeda l√©xica
        if tipo_busqueda == "LEXICA":
            carpeta_funcional = './doc/doc_scangestor/FUNCIONAL'
            terminos, resultados = busqueda_lexica_en_archivos(pregunta, carpeta_funcional)
            
            if terminos is None:
                return "‚ö†Ô∏è No se pudieron extraer t√©rminos de b√∫squeda de tu pregunta. Int√©ntalo de nuevo especificando claramente el t√©rmino que buscas."
            
            return formatear_resultados_lexicos(terminos, resultados, mostrar_fuentes)
        
        # Manejo de b√∫squeda sem√°ntica (comportamiento original)
        # 1. Buscar documentos relevantes
        results = buscar_documentos_relevantes(pregunta, categoria)
        
        # 2. Verificar si hay resultados
        if not results['documents'] or not results['documents'][0]:
            return "‚ö†Ô∏è No se encontraron documentos relevantes en la base de datos para responder tu pregunta."
        
        # 3. Construir contexto
        documentos = results['documents'][0]
        metadatas = results['metadatas'][0]
        contexto = construir_contexto(documentos, metadatas)
        
        # 4. Crear prompt especializado para preguntas funcionales
        template = """Eres un asistente experto en la aplicaci√≥n ScanGasto. 
Utiliza el siguiente contexto de documentaci√≥n para responder la pregunta del usuario de forma precisa y detallada.

Categor√≠a de la pregunta: {categoria}
Tipo de b√∫squeda: {tipo_busqueda}

CONTEXTO:
{contexto}

PREGUNTA: {pregunta}

RESPUESTA: Proporciona una respuesta clara, estructurada y basada √∫nicamente en el contexto proporcionado. Si lo ves necesario incluye ejemplos pr√°cticos o pasos a seguir.
Si el contexto no contiene informaci√≥n suficiente, ind√≠calo claramente. Puedes proponer cambios en base a las preguntas realizadas para incorporar funcionalidades nuevas."""
        
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | llm
        
        response = chain.invoke({
            "categoria": categoria,
            "tipo_busqueda": tipo_busqueda,
            "contexto": contexto,
            "pregunta": pregunta
        })
        
        # 5. Formatear respuesta con fuentes
        return formatear_respuesta_con_fuentes(response.content, metadatas, mostrar_fuentes)
        
    except Exception as e:
        return f"‚ùå Error en el agente funcional: {str(e)}"

def agente_tecnico(pregunta, categoria, tipo_busqueda, mostrar_fuentes=True):
    """
    Agente t√©cnico que busca documentos relevantes en la BBDD vectorial (sem√°ntica)
    o realiza b√∫squeda l√©xica en archivos markdown seg√∫n el tipo de b√∫squeda.
    
    Args:
        pregunta: La pregunta del usuario
        categoria: Categor√≠a de la pregunta (FUNCIONAL/TECNICA/GESTION)
        tipo_busqueda: Tipo de b√∫squeda (SEMANTICA/LEXICA)
        mostrar_fuentes: Si se deben mostrar las fuentes consultadas
    """
    try:
        # Manejo de b√∫squeda l√©xica
        if tipo_busqueda == "LEXICA":
            carpeta_tecnica = './doc/doc_scangestor/TECNICA'
            terminos, resultados = busqueda_lexica_en_archivos(pregunta, carpeta_tecnica)
            
            if terminos is None:
                return "‚ö†Ô∏è No se pudieron extraer t√©rminos de b√∫squeda de tu pregunta. Int√©ntalo de nuevo especificando claramente el t√©rmino que buscas."
            
            return formatear_resultados_lexicos(terminos, resultados, mostrar_fuentes)
        
        # Manejo de b√∫squeda sem√°ntica (comportamiento original)
        # 1. Buscar documentos relevantes
        results = buscar_documentos_relevantes(pregunta, categoria)
        
        # 2. Verificar si hay resultados
        if not results['documents'] or not results['documents'][0]:
            return "‚ö†Ô∏è No se encontraron documentos relevantes en la base de datos para responder tu pregunta."
        
        # 3. Construir contexto
        documentos = results['documents'][0]
        metadatas = results['metadatas'][0]
        contexto = construir_contexto(documentos, metadatas)
        
        # 4. Crear prompt especializado para preguntas t√©cnicas
        template = """Eres un asistente t√©cnico experto en la aplicaci√≥n ScanGasto. 
Utiliza el siguiente contexto de documentaci√≥n t√©cnica para responder la pregunta del usuario de forma precisa y detallada.

Categor√≠a de la pregunta: {categoria}
Tipo de b√∫squeda: {tipo_busqueda}

CONTEXTO:
{contexto}

PREGUNTA: {pregunta}

RESPUESTA: Proporciona una respuesta t√©cnica clara, estructurada y basada √∫nicamente en el contexto proporcionado. 
Incluye detalles t√©cnicos relevantes, arquitectura, APIs, tecnolog√≠as y patrones de implementaci√≥n cuando sea necesario.
Si el contexto no contiene informaci√≥n suficiente, ind√≠calo claramente. Si necesitas m√°s informaci√≥n puedes preguntarla.
Indica que el correo de soporte es soporte@scangasto.com."""
        
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | llm
        
        response = chain.invoke({
            "categoria": categoria,
            "tipo_busqueda": tipo_busqueda,
            "contexto": contexto,
            "pregunta": pregunta
        })
        
        # 5. Formatear respuesta con fuentes
        return formatear_respuesta_con_fuentes(response.content, metadatas, mostrar_fuentes)
        
    except Exception as e:
        return f"‚ùå Error en el agente t√©cnico: {str(e)}"

def agente_gestion(pregunta, categoria, tipo_busqueda, mostrar_fuentes=True):
    """
    Agente de gesti√≥n que busca documentos relevantes en la BBDD vectorial (sem√°ntica)
    o realiza b√∫squeda l√©xica en archivos markdown seg√∫n el tipo de b√∫squeda.
    
    Args:
        pregunta: La pregunta del usuario
        categoria: Categor√≠a de la pregunta (FUNCIONAL/TECNICA/GESTION)
        tipo_busqueda: Tipo de b√∫squeda (SEMANTICA/LEXICA)
        mostrar_fuentes: Si se deben mostrar las fuentes consultadas
    """
    try:
        # Manejo de b√∫squeda l√©xica
        if tipo_busqueda == "LEXICA":
            carpeta_gestion = './doc/doc_scangestor/GESTION'
            terminos, resultados = busqueda_lexica_en_archivos(pregunta, carpeta_gestion)
            
            if terminos is None:
                return "‚ö†Ô∏è No se pudieron extraer t√©rminos de b√∫squeda de tu pregunta. Int√©ntalo de nuevo especificando claramente el t√©rmino que buscas."
            
            return formatear_resultados_lexicos(terminos, resultados, mostrar_fuentes)
        
        # Manejo de b√∫squeda sem√°ntica (comportamiento original)
        # 1. Buscar documentos relevantes
        results = buscar_documentos_relevantes(pregunta, categoria)
        
        # 2. Verificar si hay resultados
        if not results['documents'] or not results['documents'][0]:
            return "‚ö†Ô∏è No se encontraron documentos relevantes en la base de datos para responder tu pregunta."
        
        # 3. Construir contexto
        documentos = results['documents'][0]
        metadatas = results['metadatas'][0]
        contexto = construir_contexto(documentos, metadatas)
        
        # 4. Crear prompt especializado para preguntas de gesti√≥n
        template = """Eres un asistente experto en gesti√≥n de la aplicaci√≥n ScanGasto. 
Utiliza el siguiente contexto de documentaci√≥n sobre procesos y organizaci√≥n para responder la pregunta del usuario de forma precisa y detallada.

Categor√≠a de la pregunta: {categoria}
Tipo de b√∫squeda: {tipo_busqueda}

CONTEXTO:
{contexto}

PREGUNTA: {pregunta}

RESPUESTA: Proporciona una respuesta clara, estructurada y basada √∫nicamente en el contexto proporcionado. 
Incluye informaci√≥n sobre procesos, procedimientos, organizaciones, responsabilidades, documentaci√≥n y administraci√≥n cuando sea relevante.
Si el contexto no contiene informaci√≥n suficiente, ind√≠calo claramente.
Comenta que el correo del jefe de proyecto es angel@scangasto.com.
"""
        
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | llm
        
        response = chain.invoke({
            "categoria": categoria,
            "tipo_busqueda": tipo_busqueda,
            "contexto": contexto,
            "pregunta": pregunta
        })
        
        # 5. Formatear respuesta con fuentes
        return formatear_respuesta_con_fuentes(response.content, metadatas, mostrar_fuentes)
        
    except Exception as e:
        return f"‚ùå Error en el agente de gesti√≥n: {str(e)}"

def agente_sintetizador(pregunta, respuesta_funcional, respuesta_tecnica, respuesta_gestion):
    """    Agente sintetizador que fusiona las respuestas de m√∫ltiples agentes
    en una salida coherente y estructurada.
    
    Args:
        pregunta: La pregunta original del usuario
        respuesta_funcional: Respuesta del agente funcional
        respuesta_tecnica: Respuesta del agente t√©cnico
        respuesta_gestion: Respuesta del agente de gesti√≥n
    
    Returns:
        Una respuesta sintetizada y coherente
    """
    try:
        template = """Eres un agente sintetizador experto en consolidar informaci√≥n de m√∫ltiples fuentes.

Tu tarea es analizar las respuestas de tres agentes especializados (Funcional, T√©cnico y Gesti√≥n) y crear una √öNICA respuesta coherente, bien estructurada y completa para el usuario.

PREGUNTA ORIGINAL: {pregunta}

---

**RESPUESTA DEL AGENTE FUNCIONAL:**
{respuesta_funcional}

---

**RESPUESTA DEL AGENTE T√âCNICO:**
{respuesta_tecnica}

---

**RESPUESTA DEL AGENTE DE GESTI√ìN:**
{respuesta_gestion}

---

INSTRUCCIONES PARA LA S√çNTESIS:
1. Si alguna respuesta indica "No se encontraron coincidencias", ign√≥rala y enf√≥cate en las que s√≠ tienen resultados.
2. Si todas las respuestas indican que no hay resultados, informa claramente que no se encontr√≥ informaci√≥n.
3. Organiza la informaci√≥n por categor√≠as (Funcional, T√©cnica, Gesti√≥n) SOLO si hay resultados en m√∫ltiples categor√≠as.
4. Elimina redundancias y duplicados.
5. Mant√©n las referencias a archivos y l√≠neas cuando est√©n disponibles.
6. Crea una respuesta fluida y natural, no copies y pegues literalmente.
7. Si solo hay resultados en una categor√≠a, presenta esa informaci√≥n directamente sin mencionar las otras categor√≠as.

RESPUESTA SINTETIZADA:"""
        
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | llm
        
        response = chain.invoke({
            "pregunta": pregunta,
            "respuesta_funcional": respuesta_funcional,
            "respuesta_tecnica": respuesta_tecnica,
            "respuesta_gestion": respuesta_gestion
        })
        
        return response.content
        
    except Exception as e:
        # Si falla la s√≠ntesis, devolver las respuestas organizadas manualmente
        return f"""## Resultados de b√∫squeda l√©xica

### üìã √Årea Funcional
{respuesta_funcional}

### üîß √Årea T√©cnica
{respuesta_tecnica}

### üìä √Årea de Gesti√≥n
{respuesta_gestion}

---
‚ö†Ô∏è Nota: Error al sintetizar respuestas: {str(e)}"""

# Diccionario de dispatch para selecci√≥n de agentes
AGENTES_DISPATCH = {
    "FUNCIONAL": agente_funcional,
    "TECNICA": agente_tecnico,
    "GESTION": agente_gestion
}

def chat_response(message, history, mostrar_categoria, mostrar_fuentes):
    """
    Funci√≥n principal del chat que procesa los mensajes.
    
    Args:
        message: El mensaje del usuario
        history: Historial de mensajes
        mostrar_categoria: Si se debe mostrar la categor√≠a identificada
        mostrar_fuentes: Si se deben mostrar las fuentes consultadas
    """
    if not message.strip():
        return "Por favor, escribe una pregunta."
    
    # 1. Clasificar pregunta (categor√≠a y tipo de b√∫squeda)
    clasificacion = agente_orquestador(message)
    categoria = extraer_categoria(clasificacion)
    tipo_busqueda = extraer_tipo_busqueda(clasificacion)
    
    # 2. Manejo diferenciado seg√∫n tipo de b√∫squeda
    if tipo_busqueda == "LEXICA":
        # Para b√∫squedas l√©xicas: llamar a los 3 agentes y sintetizar
        respuesta_funcional = agente_funcional(message, "FUNCIONAL", tipo_busqueda, mostrar_fuentes)
        respuesta_tecnica = agente_tecnico(message, "TECNICA", tipo_busqueda, mostrar_fuentes)
        respuesta_gestion = agente_gestion(message, "GESTION", tipo_busqueda, mostrar_fuentes)
        
        # Sintetizar las tres respuestas en una sola
        respuesta_agente = agente_sintetizador(message, respuesta_funcional, respuesta_tecnica, respuesta_gestion)
        
    else:
        # Para b√∫squedas sem√°nticas: usar el agente de la categor√≠a espec√≠fica (comportamiento original)
        agente = AGENTES_DISPATCH.get(categoria)
        
        if agente:
            respuesta_agente = agente(message, categoria, tipo_busqueda, mostrar_fuentes)
        else:
            # Para categor√≠as desconocidas
            categoria_header = f"ü§ñ **Categor√≠a identificada:** {categoria}\n\n---\n\n" if mostrar_categoria else ""
            return f"""{categoria_header}‚ö†Ô∏è Lo siento, no he podido clasificar correctamente tu pregunta. 

Int√©ntalo de nuevo con una pregunta relacionada con:
- **FUNCIONAL**: Funcionalidades, caracter√≠sticas, comportamiento de usuario, casos de uso o flujos de trabajo.
- **T√âCNICA**: Implementaci√≥n, c√≥digo, arquitectura, tecnolog√≠as, APIs, bases de datos o desarrollo.
- **GESTI√ìN**: Procesos, organizaci√≥n, documentaci√≥n, planificaci√≥n, administraci√≥n o procedimientos."""
    
    # 3. Formatear respuesta completa (con o sin categor√≠a seg√∫n el checkbox)
    if mostrar_categoria:
        tipo_busqueda_label = "üîç L√©xica (b√∫squeda en todos los documentos)" if tipo_busqueda == "LEXICA" else f"üìö Sem√°ntica - {categoria}"
        return f"""ü§ñ **Tipo de b√∫squeda:** {tipo_busqueda_label}
---
{respuesta_agente}"""
    else:
        return respuesta_agente

# Crear interfaz de Gradio
with gr.Blocks(title="IIA Capstone - ScanGasto") as demo:
    gr.Markdown("""
    # üí¨ ScanGasto - Aplicaci√≥n de gesti√≥n de tickets
    
    Puedes realizar preguntas relacionadas con la aplicaci√≥n ScanGasto. El agente clasificar√° tu pregunta en una de las siguientes categor√≠as:
    - **FUNCIONAL**: Preguntas sobre funcionalidades y casos de uso
    - **T√âCNICA**: Preguntas sobre implementaci√≥n y desarrollo
    - **GESTI√ìN**: Preguntas sobre procesos y organizaci√≥n
    Despu√©s, otro agente especializado en cada categor√≠a analizar√° la pregunta y proporcionar√° una respuesta detallada.
    """)
    
    # Checkboxes para opciones de visualizaci√≥n
    mostrar_categoria_check = gr.Checkbox(
        label="Mostrar categor√≠a",
        value=False
    )
    
    mostrar_fuentes_check = gr.Checkbox(
        label="Mostrar fuentes",
        value=False
    )
    
    chatbot = gr.ChatInterface(
        fn=chat_response,
        additional_inputs=[mostrar_categoria_check, mostrar_fuentes_check],
        title="",
        description="Escribe tu pregunta abajo:",
        examples=[
            ["¬øC√≥mo puedo registrar un ticket?"],
            ["¬øQu√© tecnolog√≠a se utiliza para comprobar un ticket con QR?"],
            ["¬øQu√© perfiles han desarrollado el m√≥dulo de consultas?"]
        ],
        cache_examples=False
    )

if __name__ == "__main__":
    demo.launch(share=False, server_name="127.0.0.1", server_port=7860)
